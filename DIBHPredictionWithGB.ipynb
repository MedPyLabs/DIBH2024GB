{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6929eac-f043-46cc-b6da-47bdb84772c3",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "babaee19-b65e-4d08-a898-31bd70fc5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from scipy.stats import mode\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import joblib\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"../dibh_clinical_only/original_training_data.csv\")\n",
    "df_internal_val = pd.read_csv(\"../dibh_clinical_only/final_internal_validation13052024.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10444a59-c608-4604-b6b5-e7fa0d0252c4",
   "metadata": {},
   "source": [
    "## 2. Data Prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ffd171-231f-46b1-beb4-c0db9af9409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "data_day1 = df[df['day'] == 1]\n",
    "X = data_day1.drop(['crnumber', 'day', 'DIBH_Y0N1'], axis=1)\n",
    "y = data_day1['DIBH_Y0N1']\n",
    "X_t, X_v, y_t, y_v = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "data_day1_val = df_internal_val[df_internal_val['day'] == 1]\n",
    "X_int_val = data_day1_val.drop(['crnumber', 'day', 'DIBH_Y0N1'], axis=1)\n",
    "y_int_val = data_day1_val['DIBH_Y0N1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d060677d-700b-4069-b12c-00b2e59ad98d",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf13fad-ec0c-48cd-abdc-7834f099dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature categories\n",
    "categorical_features = ['al_N0_Y1', 'surgery_BCS1MRM2', 'chemo_No0_Adj1_NAdj2', 'comorb_no0_cardio1_others2']\n",
    "continuous_features = ['age', 'BMI', 'ul_amp', 'll_amp', 'average_amp', 'ahd']\n",
    "\n",
    "# Create pipelines for numerical and categorical features\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, continuous_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176169eb-292c-4cbe-b7d2-80e7e05157e8",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded7550-14ce-475e-a7cc-3a853fe23047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "    }\n",
    "    gb_clf = make_pipeline(preprocessor, GradientBoostingClassifier(**params, random_state=42))\n",
    "    cv_roc_auc = cross_val_score(gb_clf, X_t, y_t, cv=10, scoring='roc_auc').mean()\n",
    "    return cv_roc_auc\n",
    "\n",
    "# Create and optimize the study\n",
    "storage_name = \"sqlite:///db.sqlite3\"\n",
    "sampler = TPESampler(seed=72)\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler, storage=storage_name, study_name=\"GB_one_day_assessment_final04\")\n",
    "study.optimize(objective, n_trials=1000)\n",
    "\n",
    "# Load the best parameters\n",
    "loaded_study = optuna.create_study(study_name=\"GB_one_day_assessment_final01\", storage=storage_name, load_if_exists=True)\n",
    "best_params = loaded_study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdb83c6-25b3-4fbc-a85b-f13cc7be9cf9",
   "metadata": {},
   "source": [
    "## 5. Bootstrap Sampling and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128026de-9bc1-4b53-b004-858cd86da344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store results\n",
    "top_models = []\n",
    "fraction_of_positives_calibrated_list = []\n",
    "mean_predicted_value_calibrated_list = []\n",
    "fraction_of_positives_non_calibrated_list = []\n",
    "mean_predicted_value_non_calibrated_list = []\n",
    "results = []\n",
    "\n",
    "# Perform bootstrap sampling and model training\n",
    "for i in range(1000):\n",
    "    X_resampled, y_resampled = resample(X_t, y_t)\n",
    "    X_train_resampled, X_val_resampled, y_train_resampled, y_val_resampled = train_test_split(X_resampled, y_resampled, test_size=0.2)\n",
    "\n",
    "    gb_pipeline = make_pipeline(preprocessor, GradientBoostingClassifier(**best_params, random_state=42))\n",
    "    gb_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "    y_proba_non_calibrated = gb_pipeline.predict_proba(X_v)[:, 1]\n",
    "\n",
    "    best_gb_pipeline = make_pipeline(preprocessor, GradientBoostingClassifier(**best_params, random_state=42))\n",
    "    model = CalibratedClassifierCV(best_gb_pipeline, method='isotonic', cv=10)\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    y_proba_calibrated = model.predict_proba(X_v)[:, 1]\n",
    "\n",
    "    fraction_of_positives_non_calibrated, mean_predicted_value_non_calibrated = calibration_curve(y_v, y_proba_non_calibrated, n_bins=6)\n",
    "    if len(fraction_of_positives_non_calibrated) == 6:\n",
    "        fraction_of_positives_non_calibrated_list.append(fraction_of_positives_non_calibrated)\n",
    "        mean_predicted_value_non_calibrated_list.append(mean_predicted_value_non_calibrated)\n",
    "\n",
    "    fraction_of_positives_calibrated, mean_predicted_value_calibrated = calibration_curve(y_v, y_proba_calibrated, n_bins=6)\n",
    "    if len(fraction_of_positives_calibrated) == 6:\n",
    "        fraction_of_positives_calibrated_list.append(fraction_of_positives_calibrated)\n",
    "        mean_predicted_value_calibrated_list.append(mean_predicted_value_calibrated)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_v, y_proba_calibrated)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    y_pred_optimal = (y_proba_calibrated >= optimal_threshold).astype(int)\n",
    "\n",
    "    model_info = {\n",
    "        'model': model,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'thresholds': thresholds,\n",
    "        'roc_auc': roc_auc,\n",
    "        'optimal_threshold': optimal_threshold,\n",
    "        'accuracy': accuracy_score(y_v, y_pred_optimal),\n",
    "        'precision': precision_score(y_v, y_pred_optimal),\n",
    "        'recall': recall_score(y_v, y_pred_optimal),\n",
    "        'f1_score': f1_score(y_v, y_pred_optimal),\n",
    "        'confusion_matrix': confusion_matrix(y_v, y_pred_optimal),\n",
    "        'classification_report': classification_report(y_v, y_pred_optimal)\n",
    "    }\n",
    "\n",
    "    model_info_without_model = {key: value for key, value in model_info.items() if key != 'model'}\n",
    "    results.append(model_info_without_model)\n",
    "\n",
    "    if len(top_models) < 10:\n",
    "        top_models.append(model_info)\n",
    "    else:\n",
    "        min_index = min(range(len(top_models)), key=lambda x: (top_models[x]['roc_auc'], top_models[x]['recall']))\n",
    "        if roc_auc > top_models[min_index]['roc_auc']:\n",
    "            top_models[min_index] = model_info\n",
    "\n",
    "    if (i + 1) % 25 == 0:\n",
    "        print(f\"Bootstrap sample no. {i + 1} ------ Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c67d2c8-65af-456c-bc27-57b4602f5aa7",
   "metadata": {},
   "source": [
    "## 6. Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c16ca-fa32-40a0-aae1-65eaa9098250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "metric_summary = df_results[['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']].agg(['mean', 'std', 'min', 'max'])\n",
    "metric_summary.loc['95% CI lower'] = metric_summary.loc['mean'] - 1.96 * metric_summary.loc['std']\n",
    "metric_summary.loc['95% CI upper'] = metric_summary.loc['mean'] + 1.96 * metric_summary.loc['std']\n",
    "\n",
    "print(metric_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06edde4-1bb7-42a7-b61d-3179df5c6090",
   "metadata": {},
   "source": [
    "## 7. Saving the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2029ef1-72d2-4b12-b6be-367cd546d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the top models and preprocessor\n",
    "model_folder_path = '../saved_models/one_day_gb_top10'\n",
    "\n",
    "if os.path.exists(model_folder_path):\n",
    "    shutil.rmtree(model_folder_path)\n",
    "os.makedirs(model_folder_path)\n",
    "\n",
    "for i, m in enumerate(top_models):\n",
    "    model_path = os.path.join(model_folder_path, f'top_model_{i+1}.joblib’)\n",
    "joblib.dump(m[‘model’], model_path)\n",
    "\n",
    "preprocessor_path = os.path.join(model_folder_path, ‘preprocessor.joblib’)\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "\n",
    "print(f”All top models have been saved to folder: {model_folder_path}”)\n",
    "print(f”Preprocessor has been saved as {preprocessor_path}”)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82790dc0-d04d-4a74-859a-d40c6ff1e9c2",
   "metadata": {},
   "source": [
    "## 8. Creating an Ensemble Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13852ac-371f-42fb-ae6f-a10b6b6786ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ensemble classifier\n",
    "class ThresholdedEnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.mean([model.predict_proba(X)[:, 1] for model, _ in self.models], axis=0)\n",
    "        return probabilities\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.array([model.predict_proba(X)[:, 1] >= threshold for model, threshold in self.models]).astype(int).T\n",
    "        final_predictions = mode(predictions, axis=1)[0].flatten()\n",
    "        return final_predictions\n",
    "\n",
    "# Save the ensemble classifier\n",
    "ensemble_models = [(m['model'], m['optimal_threshold']) for m in top_models]\n",
    "ensemble_classifier = ThresholdedEnsembleClassifier(ensemble_models)\n",
    "\n",
    "joblib_file = '../saved_models/one_day_gb_top10/ensemble_classifier_gb.joblib'\n",
    "joblib.dump(ensemble_classifier, joblib_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a6238-407f-41b4-aaee-d92c48f33eb1",
   "metadata": {},
   "source": [
    "## 9. Evaluation on Internal Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204d265-3c76-451c-988a-536293f44b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the ensemble classifier on internal validation data\n",
    "ensemble_classifier = joblib.load(joblib_file)\n",
    "y_pred = ensemble_classifier.predict(X_int_val)\n",
    "y_proba = ensemble_classifier.predict_proba(X_int_val)\n",
    "\n",
    "print(\"Ensemble Accuracy:\", accuracy_score(y_int_val, y_pred))\n",
    "print(\"Ensemble ROC AUC:\", roc_auc_score(y_int_val, y_proba))\n",
    "print(\"Ensemble Precision:\", precision_score(y_int_val, y_pred))\n",
    "print(\"Ensemble Recall:\", recall_score(y_int_val, y_pred))\n",
    "print(\"Ensemble F1 Score:\", f1_score(y_int_val, y_pred))\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_int_val, y_pred)\n",
    "\n",
    "# Define the class labels\n",
    "class_labels = ['DIBH', 'NonDIBH']\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_int_val, y_pred, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e9e24-d1d3-48a7-8fc0-e795f3dad74c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2596f2a-40ea-4f60-99b7-c0f5923f2c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d99e57-0c24-46a0-b09a-3ab73709586f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d1dc1-07e2-4290-bd3a-e60dcfcd349c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef352943-9af1-4785-abe9-4a9dc0d32413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e7a7d6-d510-499a-be5b-7668682531f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f543f32-d05f-465d-98f6-aa124f90196f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe753b10-158e-4980-8cce-360bc3ff6ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c40161-9c25-4b26-be83-d8a84c511365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306542bc-9404-4b66-b678-66d4f62d2e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e1797-afbe-4646-bf4a-822a02cc70bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f800ddf-023a-422b-9170-fa2669015f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc61d7-6445-4eba-9401-302f9639e16e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
